training_protocols:
  GCN:
    optimizer_name: "adam"
    learning_rate: 0.001
    weight_decay: 0.001
    warm_up: true
    warm_up_epochs: 50
    n_epochs: 200
    early_stopping: false
    patience: 20

  GAT:
    optimizer_name: "adam"
    learning_rate: 0.0001 #0.0001
    weight_decay: 0.0001
    warm_up: true
    warm_up_epochs: 50
    n_epochs: 200
    early_stopping: false
    patience: 20

  SAGE:
    optimizer_name: "adam"
    learning_rate: 0.001
    weight_decay: 0.001
    warm_up: true
    warm_up_epochs: 50
    early_stopping: false
    n_epochs: 200
    patience: 20

  APPNPNet:
    optimizer_name: "adam"
    learning_rate: 0.01
    weight_decay: 0.0001
    warm_up: true
    warm_up_epochs: 50
    early_stopping: false
    n_epochs: 200
    patience: 20
