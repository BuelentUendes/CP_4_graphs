training_protocols:
  GCN:
    optimizer_name: "adam"
    learning_rate: 0.01
    weight_decay: 0.001
    warm_up: true
    warm_up_epochs: 50
    n_epochs: 200
    early_stopping: true
    patience: 10
    verbose: true

  GAT:
    optimizer_name: "adam"
    learning_rate: 0.01
    weight_decay: 0.01
    warm_up: true
    warm_up_epochs: 50
    n_epochs: 200
    early_stopping: true
    patience: 10
    verbose: true

  SAGE:
    optimizer_name: "adam"
    learning_rate: 0.001
    weight_decay: 0.01
    warm_up: true
    warm_up_epochs: 50
    early_stopping: true
    n_epochs: 200
    patience: 10
    verbose: true

  APPNPNet:
    optimizer_name: "adam"
    learning_rate: 0.01
    weight_decay: 0.0001
    warm_up: true
    warm_up_epochs: 50
    early_stopping: true
    n_epochs: 200
    patience: 10
    verbose: true